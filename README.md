# Challenge 
For dollar shave club

## PYTHON Programms
	HashAlgo.py	-- Hashing Algorithm	   
	LinkedList.py 	-- Linked List problem
	sparketl.py	-- Spark Application

## DATA SCIENCE - MACHINE LERNING
Note about the Machine Learning Models generated by Training and testing provided marketing data
by Jean Claude Lemoyne
============================
Used library: Weka (java 1.8)
Used Weka Platform to generate results and models
Selected the 4 best model in terms of performance - see results
REMARK:
	However the best results were by the Logistic Model 
	This is to be expected since it's a binary classification
INTERPRETATION:
 	There is an imbalance betwen the 'yes' and 'no' predicting 'yes' is far less accurate
	judging by the F-measure for each target feature attribute
	Overall accuracy is excellent ~ 90% but the Kappa statistic indicates a poor performance
	More feature engineering is require by regularization (i.e. eliminate the features that cause the degradation 
	of the 'yes'
	In general the results are acceptable and provide an expected 60% lift in targetting

HOW TO USE THE GENERATED MODELS BY A PREDICTIVE ENGINE
======================================================
The generated models are binary file or serialized objects that are used by a java program
which import the Weka library (weka.jar).
The program should deserialized the model file into a Classifier object
Use the classifier object to predict and generate an associated probability vector (Prob(yes), Prob(no)) 

check the following files for results:
	LMT_results.txt 	-- Logistic Model Tree      
	Logistic_results.txt  	-- Logistic
	NBayes_results.txt   	-- Naive Bayesian
	SMO_results.txt		-- Support Vector Machine

check the following files for models:
	LMT_model.bin.model      
	Logistic_model.bin.model 
	NBayes_model.bin.model   
	SMO_model.bin.model

PREDICTIONS:
	Logistic_Visualize.csv from Logistic_Visualize.arff generated by Weka (ARFF is Weka format)
	
	Note: 	Weka prepends (before the responded) 2 columns (features) 'prediction and 'predicted
		'prediction is the numerical value of the predicted.
	
	Missing Data: 	shown as ? Missing data is handled by sparse data instance vectors (instance is a row)
			the whole advantage of Machine Learning techniques is the handling of missing data.
			From a feature point view this means that the samples are different for each feature.
			This does not prevent the building of predictive models.
	
	About the Models: 	their decsriptions could be found in ML litterature - the 4 ones I picked from about 30 
				are known and established. For instance the SMO is an effcient algorithm to solve
				the problem of separaing n-dimentional (here 21-dimetional) vectors labeled 'yes' or 
				'no'. The problem is to find a hyperplane that separates optimally (maximize margin)
				the 2 sets of points. Prediction is checking on which side of the hyperplane an
				unpredicted instance falls.
				I let the reader discover the Bayesian technique (conditional probabilities bases on
				frequencies) etc.
  		
	About Testing: 	All models were tested by being regularized (10-fold, one out for test). All data is shuffled. 
			No dropout which is part of feature engineering or use of Stochastic Gradiant Descent. 
			Haven't got the time to try all this in a typical Perceptron modeling. 

## ARCHITECTURE OF A SOLUTION - USE CASE abc.com company (This is a rough draft)

	Problem: 	abc.com website should ideentify (quickly within 20ms) if Mr.X is a interested.
	Solution: 	Recommendation Engine that communicates with the Ad Server showing a ranked list
			of product prefences. The recommendation engine is based on a Machine Learning mode
			that predicts the list of preferences based on BI (Behavioral Intelligence) vectors 
			of instances of BI data related to the user. User may be registered or unregistered.
			if unregistered the RE(Recommendation Engine) will use prior probabilities that were
			collected beforehand

	Data:		Prior Probabilities:
				1) General - no prior information is known about the user
					for instance what is the frequency for someone to buy product P from
					a large diversified sample (age group, gender, income, etc)
				2) Partial - We have some clues (age, gender, etc)
			
			Behavioral Intelligence:
				1) From user session analysis of browsing pages (pages linked list)
				2) From background / registration (note that this information is not really accurate 
				   and contains lots of noise)
					typically: demographic, geographic and socio-economic info such as
						gender, age group, income group, zip code, perhaps credit info, etc.
						note: most of the time the info is missing
				3) From history - if keeping track of history:
					locally: through cookies
					in general if registered user - difficult to tell when same user 
						unless preserved in cookie
				4) From social media - require NLP analysis	
				
			
	
